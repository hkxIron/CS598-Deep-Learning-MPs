{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Neural Networks on MNIST dataset\n",
    "#### Rachneet Kaur, rk4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Library imports\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import copy\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the MNIST training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Training set shape = (60000, 784)\n",
      "MNIST Test set shape = (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Path for the dataset file\n",
    "path = 'C:/Users/Rachneet Kaur/Desktop/UIUC/UIUC Fall 2018/IE 534 CS 598 Deep Learning/HW/Datasets/'\n",
    "\n",
    "#MNIST dataset\n",
    "MNIST_data = h5py.File(path + 'MNISTdata.hdf5', 'r')\n",
    "\n",
    "#Training set\n",
    "x_train = np.float32(MNIST_data['x_train'][:]) #x_train.shape = (60000, 784)\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:,0])) #y_train.shape = (60000, 1)\n",
    "print ('MNIST Training set shape =', x_train.shape)\n",
    "\n",
    "#Testing set\n",
    "x_test = np.float32( MNIST_data['x_test'][:]) #x_test.shape = (10000, 784)\n",
    "y_test = np.int32( np.array( MNIST_data['y_test'][:,0])) #y_test.shape = (10000, 1)\n",
    "print ('MNIST Test set shape =', x_test.shape)\n",
    "\n",
    "MNIST_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the softmax function for the output layer\n",
    "def softmax_function(z):\n",
    "    Z = np.exp(z)/np.sum(np.exp(z))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the activation function and it's derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the activation function and it's derivative if flag derivative = 1 \n",
    "def activation(z, derivative = 0):\n",
    "    if (derivative == 1):\n",
    "        return 1.0-np.tanh(z)**2 #Derivative of tanh(z)\n",
    "    else:\n",
    "        return np.tanh(z) #tanh(z) as activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the function to compute the accuracy of the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to compute the accuracy on the testing dataset\n",
    "def compute_accuracy(x_series, y_series, model):\n",
    "    total_correct = 0\n",
    "    for index in range(len(x_series)):\n",
    "        y = y_series[index] #True label \n",
    "        x = x_series[index][:] #Input \n",
    "        Z, H, p = forward(x, y, model)\n",
    "        prediction = np.argmax(p) #Predicting the label based on the input\n",
    "        if (prediction == y): #Checking if True label == Predicted label\n",
    "            total_correct += 1\n",
    "    accuracy = total_correct/np.float(len(x_series))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape parameters for the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shape parameters for the layers\n",
    "num_inputs = x_train.shape[1] \n",
    "#number of input features for each image = 28*28 = 784 = d\n",
    "num_outputs = 10 #number of output classes = k\n",
    "num_hidden = 140 #number of hidden units in the hidden layer = d_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the parameters for the Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing the parameters for the Neural Network model\n",
    "model = {}\n",
    "model['W'] = np.random.randn(num_hidden, num_inputs) / np.sqrt(num_inputs)\n",
    "#d_H*d dimensional \n",
    "model['b1'] = np.random.randn(num_hidden, 1)\n",
    "#d_H dimensional \n",
    "model['C'] = np.random.randn(num_outputs, num_hidden) / np.sqrt(num_hidden)\n",
    "#k*d_H dimensional \n",
    "model['b2'] = np.random.randn(num_outputs, 1) \n",
    "#k*1 dimensional\n",
    "\n",
    "model_grads = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Defining the forward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the forward step of the Neural Network model\n",
    "def forward(x, y, model): \n",
    "    x = x.reshape(-1,1)\n",
    "    y = y.reshape(-1,1)\n",
    "    Z = np.dot(model['W'], x) + model['b1'] #Z = Wx + b_1 - d_H dimensional\n",
    "    H = activation(Z) #H = activation(Z) - d_H dimensional\n",
    "    U = np.dot(model['C'], H) + model['b2'] #U = CH +b_2 - k dimensional \n",
    "    prob_dist = softmax_function(U) \n",
    "    #Prob_distribution of classes = F_softmax(U) - k dimensional \n",
    "    return Z, H, prob_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the backpropogation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the backpropogation step of the Neural Network model\n",
    "def backward(x, y, Z, H, prob_dist, model, model_grads):\n",
    "    x = x.reshape(-1,1)\n",
    "    y = y.reshape(-1,1)\n",
    "    dZ = -1.0*prob_dist\n",
    "    dZ[y] = dZ[y] + 1.0 \n",
    "    # Gradient(log(F_softmax)) wrt U = Indicator Function - F_softmax\n",
    "    model_grads['b2'] = dZ #Gradient(b_2) = Gradient(log(F_softmax)) wrt U\n",
    "    #k*1 dimensional\n",
    "    model_grads['C'] = np.dot(dZ, np.transpose(H)) #Gradient(C) = (Gradient(log(F_softmax)) wrt U)*H^(Transpose)\n",
    "    #k*d_H dimensional \n",
    "    delta = np.dot(np.transpose(model['C']), dZ) #delta = Gradient(H) = C^(Transpose)*(Gradient(log(F_softmax)) wrt U)\n",
    "    #d_H dimensional \n",
    "    model_grads['b1'] = np.multiply(delta, activation(Z, 1)) #Gradient(b_1) = delta.derivative of activation(Z)\n",
    "    #d_H dimensional\n",
    "    model_grads['W'] = np.dot(np.multiply(delta, activation(Z, 1)), np.transpose(x)) #Gradient(W) = delta.derivative of activation(Z) * X^(Transpose)\n",
    "    #d_H*d dimensional \n",
    "    return model_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch  0 , accuracy in training set =  0.92375\n",
      "In epoch  1 , accuracy in training set =  0.9660666666666666\n",
      "In epoch  2 , accuracy in training set =  0.9765\n",
      "In epoch  3 , accuracy in training set =  0.98265\n",
      "In epoch  4 , accuracy in training set =  0.9856\n",
      "In epoch  5 , accuracy in training set =  0.9890166666666667\n",
      "In epoch  6 , accuracy in training set =  0.9937\n",
      "In epoch  7 , accuracy in training set =  0.9954833333333334\n",
      "In epoch  8 , accuracy in training set =  0.9956\n",
      "In epoch  9 , accuracy in training set =  0.9963166666666666\n",
      "In epoch  10 , accuracy in training set =  0.99685\n",
      "In epoch  11 , accuracy in training set =  0.997\n",
      "In epoch  12 , accuracy in training set =  0.99695\n",
      "In epoch  13 , accuracy in training set =  0.9972833333333333\n",
      "In epoch  14 , accuracy in training set =  0.9976666666666667\n",
      "Estimated time to train the model =  903.1648941040039  seconds\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "LR = .01\n",
    "num_epochs = 15 #No. of epochs we are training the model\n",
    "#Stochastic Gradient Descent algorithm\n",
    "for epochs in range(num_epochs):\n",
    "    #Defining the learning rate based on the no. of epochs\n",
    "    if (epochs > 5):\n",
    "        LR = 0.001\n",
    "    if (epochs > 10):\n",
    "        LR = 0.0001\n",
    "    if (epochs > 15):\n",
    "        LR = 0.00001\n",
    "        \n",
    "    #Updating the parameters based on the SGD algorithm \n",
    "    total_correct = 0\n",
    "    for n in range(len(x_train)):\n",
    "        n_random = randint(0,len(x_train)-1)\n",
    "        y = y_train[n_random]\n",
    "        x = x_train[n_random][:]\n",
    "        Z, H, prob_dist = forward(x, y, model)\n",
    "        prediction = np.argmax(prob_dist)\n",
    "        if (prediction == y):\n",
    "            total_correct += 1\n",
    "        model_grads = backward(x, y, Z, H, prob_dist, model, model_grads)\n",
    "        model['W'] = model['W'] + LR*model_grads['W'] # Updating the parameters W, b_1, C and b_2 via the SGD step\n",
    "        model['b1'] = model['b1'] + LR*model_grads['b1']\n",
    "        model['C'] = model['C'] + LR*model_grads['C']\n",
    "        model['b2'] = model['b2'] + LR*model_grads['b2']\n",
    "    print('In epoch ', epochs, ', accuracy in training set = ', total_correct/np.float(len(x_train)))\n",
    "time2 = time.time()\n",
    "print('Estimated time to train the model = ', time2-time1, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in testing set = 0.979\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = compute_accuracy(x_test, y_test, model)\n",
    "print('Accuracy in testing set =', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
