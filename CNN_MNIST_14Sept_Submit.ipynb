{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Convolution Neural Networks on MNIST dataset\n",
    "#### Rachneet Kaur, rk4\n",
    "##### Accuracy in testing set = 0.9735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Library imports\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import copy\n",
    "from random import randint\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the MNIST training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Training set shape = (60000, 28, 28)\n",
      "MNIST Test set shape = (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#Path for the dataset file\n",
    "path = 'C:/Users/Rachneet Kaur/Desktop/UIUC/UIUC Fall 2018/IE 534 CS 598 Deep Learning/HW/Datasets/'\n",
    "\n",
    "#MNIST dataset\n",
    "MNIST_data = h5py.File(path + 'MNISTdata.hdf5', 'r')\n",
    "d = 28 #number of input features for each image = 28*28  = d * d\n",
    "\n",
    "#Training set\n",
    "x_train = np.float32(MNIST_data['x_train'][:]) #x_train.shape = (60000, 784)\n",
    "x_train = np.array([x.reshape(d,d) for x in x_train]) #Reshaping the image in a matrix format\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:,0])) #y_train.shape = (60000, 1)\n",
    "y_train = [y.reshape(-1,1) for y in y_train]\n",
    "print ('MNIST Training set shape =', x_train.shape)\n",
    "\n",
    "#Testing set\n",
    "x_test = np.float32( MNIST_data['x_test'][:]) #x_test.shape = (10000, 784)\n",
    "x_test = np.array([x.reshape(d,d) for x in x_test]) #Reshaping the image in a matrix format\n",
    "y_test = np.int32( np.array( MNIST_data['y_test'][:,0])) #y_test.shape = (10000, 1)\n",
    "y_test = [y.reshape(-1,1) for y in y_test]\n",
    "print ('MNIST Test set shape =', x_test.shape)\n",
    "MNIST_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the softmax function for the output layer\n",
    "def softmax_function(z):\n",
    "    softmax = np.exp(z)/np.sum(np.exp(z))\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the convolution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the convolution function\n",
    "def convolution(X, K, iteratable, d_y, d_x): # X - image, K - filter, (d_x, d_y, channel) = Dimensions of filter K\n",
    "    conv_Z = np.array([np.tensordot(K[:, :, tuple_ijk[2]], X[tuple_ijk[0]:tuple_ijk[0]+d_y,tuple_ijk[1]:tuple_ijk[1]+d_x], axes = ((0,1), (0,1))) for tuple_ijk in iteratable])\n",
    "    return conv_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the activation function and it's derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the activation function and it's derivative if flag derivative = 1 \n",
    "def activation(Z, derivative = 0):\n",
    "    if (derivative == 1):\n",
    "        return 1.0-np.tanh(Z)**2 #Derivative of tanh(z) applied elementwise \n",
    "    else:\n",
    "        return np.tanh(Z) #tanh(z) as activation function applied elementwise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the function to compute the accuracy of the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to compute the accuracy on the testing dataset\n",
    "def compute_accuracy(x_series, y_series, model):\n",
    "    total_correct = 0\n",
    "    for index in range(len(x_series)):\n",
    "        y = y_series[index] #True label \n",
    "        x = x_series[index][:] #Input \n",
    "        Z, H, p = forward(x, y, model)\n",
    "        prediction = np.argmax(p) #Predicting the label based on the input\n",
    "        if (prediction == y): #Checking if True label == Predicted label\n",
    "            total_correct += 1\n",
    "    accuracy = total_correct/np.float(len(x_series))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape parameters for the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shape parameters for the layers\n",
    "num_outputs = 10 #number of output classes = k\n",
    "#Dimensions for the Kernel = d_y * d_x * C\n",
    "d_y = 5\n",
    "d_x = 5\n",
    "channel = 5 #No. of channels C\n",
    "#dimensions of hidden units in the hidden layer\n",
    "num_hidden_x = d-d_y+1 \n",
    "num_hidden_y = d-d_x+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the parameters for the Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing the parameters for the Convolution Neural Network model\n",
    "model = {}\n",
    "model['K'] = np.random.randn(d_y, d_x, channel)/np.sqrt(d_x*d_y)\n",
    "#K = d_y *d_x * C dimensional \n",
    "model['W'] = np.random.randn(num_outputs, num_hidden_x, num_hidden_y, channel)/np.sqrt(num_hidden_x*num_hidden_y)\n",
    "#W = k*(d-d_y+1)*(d-d_x+1)*C dimensional \n",
    "model['b'] = np.random.randn(num_outputs, 1)\n",
    "#b = k*1 dimensional \n",
    "\n",
    "model_grads = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the iteratables for the convolution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the iteratables for the convolution function\n",
    "l1= range(num_hidden_x)\n",
    "l2 = range(num_hidden_y)\n",
    "l3 = range(channel)\n",
    "iteratable_forward = list(itertools.product(l1, l2, l3))\n",
    "\n",
    "i1= range(d_y)\n",
    "i2 = range(d_x)\n",
    "iteratable_backward= list(itertools.product(i1, i2, l3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the forward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the forward step of the Convolution Neural Network model\n",
    "def forward(x, y, model): \n",
    "    Z = convolution(x, model['K'], iteratable_forward, d_y, d_x).reshape(num_hidden_x, num_hidden_y, channel) \n",
    "    #Z = X convolution K = d-d_y+1*d-d_x+1*channel dim.\n",
    "    H = activation(Z) #H = activation(Z) - (d-d_y+1)*(d-d_x+1)*channel dimensional\n",
    "    U = np.tensordot(model['W'], H, axes = ((1,2,3),(0,1,2))).reshape(-1,1) + model['b'] #U = W.H + b - k dimensional \n",
    "    prob_dist = softmax_function(U) #Prob_distribution of classes = F_softmax(U) - k dimensional \n",
    "    return Z, H, prob_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the backpropogation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the backpropogation step of the Convolution Neural Network model\n",
    "def backward(x, y, Z, H, prob_dist, model, model_grads):\n",
    "    dZ = -1.0*prob_dist\n",
    "    dZ[y] = (dZ[y] + 1.0) \n",
    "    dZ = -dZ\n",
    "    # Gradient(log(F_softmax)) wrt U = Indicator Function - F_softmax\n",
    "    model_grads['b'] = dZ #Gradient(b) = Gradient(log(F_softmax)) wrt U\n",
    "    #Gradient_b = k*1 dimensional\n",
    "    model_grads['W'] = np.tensordot(dZ.T, H, axes = 0)[0]\n",
    "    #Gradient_W = k*(d-d_y+1)*(d-d_x+1)*C dimensional \n",
    "    delta = np.tensordot(dZ.T, model['W'], axes = 1)[0] #delta_{i,j,p} = Gradient(H) = (Gradient(log(F_softmax)) wrt U)*W_{:,i,j,p}\n",
    "    #delta = (d-d_y+1)* (d-d_x+1)* C dimensional \n",
    "    model_grads['K'] =  convolution(x, np.multiply(delta, activation(Z, 1)), iteratable_backward, d-d_y+1, d-d_x+1).reshape(d_y, d_x, channel)\n",
    "    #Gradient(W) = X convolution delta.derivative of activation(Z) \n",
    "    #Using the dimensions of np.multiply(delta, activation(Z, 1)) as an input to the convolution function\n",
    "    #model_grads['K'] = d_y*d_x*C dimensional \n",
    "    return model_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch  0 , accuracy in training set =  0.9292833333333334\n",
      "In epoch  1 , accuracy in training set =  0.9618166666666667\n",
      "In epoch  2 , accuracy in training set =  0.96655\n",
      "In epoch  3 , accuracy in training set =  0.9711\n",
      "In epoch  4 , accuracy in training set =  0.9726333333333333\n",
      "In epoch  5 , accuracy in training set =  0.9764333333333334\n",
      "In epoch  6 , accuracy in training set =  0.98625\n",
      "In epoch  7 , accuracy in training set =  0.9881666666666666\n",
      "In epoch  8 , accuracy in training set =  0.9894333333333334\n",
      "In epoch  9 , accuracy in training set =  0.9901\n",
      "Accuracy in testing set = 0.9735\n"
     ]
    }
   ],
   "source": [
    "LR = .01\n",
    "num_epochs = 10 #No. of epochs we are training the model\n",
    "\n",
    "#Stochastic Gradient Descent algorithm\n",
    "for epochs in range(num_epochs):\n",
    "    time1 = time.time()\n",
    "    #Defining the learning rate based on the no. of epochs\n",
    "    if (epochs > 5):\n",
    "        LR = 0.001\n",
    "    if (epochs > 10):\n",
    "        LR = 0.0001\n",
    "    if (epochs > 15):\n",
    "        LR = 0.00001\n",
    "        \n",
    "    #Updating the parameters based on the SGD algorithm \n",
    "    total_correct = 0\n",
    "    for n in range(len(x_train)):\n",
    "        n_random = randint(0,len(x_train)-1) #SGD step\n",
    "        y = y_train[n_random]\n",
    "        x = x_train[n_random][:]\n",
    "        Z, H, prob_dist = forward(x, y, model)\n",
    "        prediction = np.argmax(prob_dist)\n",
    "        if (prediction == y):\n",
    "            total_correct += 1\n",
    "        model_grads = backward(x, y, Z, H, prob_dist, model, model_grads)\n",
    "        model['W'] = model['W'] - LR*model_grads['W'] # Updating the parameters W, b, and K via the SGD step\n",
    "        model['b'] = model['b'] - LR*model_grads['b']\n",
    "        model['K'] = model['K'] - LR*model_grads['K']\n",
    "    print('In epoch ', epochs, ', accuracy in training set = ', total_correct/np.float(len(x_train)))\n",
    "\n",
    "#Calculating the test accuracy \n",
    "test_accuracy = compute_accuracy(x_test, y_test, model)\n",
    "print('Accuracy in testing set =', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
